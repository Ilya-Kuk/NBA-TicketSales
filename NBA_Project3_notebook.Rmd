---
title: "NBA Project"
author: "Ilya Kukovitskiy"
date: "April 30, 2018"
output: html_document
---
```{r,echo=FALSE}
set.seed(1)
library(tree)
library(randomForest)
library(gbm)
#library(plotly)
```

**Data Preprocessing**

Reading in and storing dataset from file
```{r}
d <- read.csv("nbasample2GrpPlus0429.csv",header=T, sep=",")
```

Removing ax_ and la_ variales, as requested
```{r}
a <- colnames(d)
a
AX <- grepl("ax_",a) #seeing which values have prefix ax_
AXN <- c() #creating temporary vector recording positions
for(i in 1:length(a)){ #getting rejected values
  if(AX[i]){
    AXN <- c(AXN,i)
  }
}
AXN
d <- d[,-AXN] #removing rejected values
a <- colnames(d)
a #no ax_ variables
grepl("ax_",a)
LA <- grepl("la_",a)
LAN <- c() #creating temporary vector recording positions
for(i in 1:length(a)){ #getting rejected values
  if(LA[i]){
    LAN <- c(LAN,i)
  }
}
LAN
d <- d[,-LAN] #removing rejected values
a <- colnames(d)
a
grepl("la_",a) #no la_ variables
#Cleaning up Environment
rm(AX,AXN,LA,LAN)
```

Removing other Variables
```{r}
#closest_arena and closest_team are identical - removing closest_arena
x <- match("closest_arena",a)
x
d <- d[,-x]
a <- colnames(d)
match("closest_arena",a) #successfully removed

names(Filter(is.factor, d))#tree() can only handle 32 levels on factor predictors

#zip_team and zip_arena have 56 levels - too many to grow trees on - removing both
x <- match(c("zip_team","zip_arena"),a)
x
d <- d[,-x]
a <- colnames(d)
match(c("zip_team","zip_arena"),a) #successfully removed
#email_domain has 1468 levels - too many.. -removing
x <- match(c("email_domain"),a)
x
d <- d[,-x]
a <- colnames(d)
match(c("email_domain"),a) #successfully removed
#source_date has 3773 levels - too many.. -removing
x <- match(c("source_date"),a)
x
d <- d[,-x]
a <- colnames(d)
match(c("source_date"),a) #successfully removed
```

**Making a training/test set vector**

```{r}
train <- sample(1:nrow(d), nrow(d)*(.8))
```


**Viewing Response Variables**

```{r}
POST <- grepl("post_",a)
POSTN <- c()
for(i in 1:length(a)){ #getting rejected values
  if(POST[i]){
    POSTN <- c(POSTN,i)
  }
}
POSTN
a[POSTN]
```

**Given this dataset, a few useful questions can be answered:**
A - Who buys tickets?
Successful or unsuccessful marketting strategies can be identified.

B - How many tickets do people buy?
Besides success of strategy; if a person is likely to buy more tickets than they have purchased, they can likely be sold more tickets.

C - How much money do people spend on tickets?
Same rationale as B, but with dollar-value rather than number of games.

D - Who buys tickets at which price?
Better seats can be marketted more to those who are more likely to buy them.

**A - Who buys tickets?**

Removing other response variables besides post_tickets_flag
```{r}
Q <- match('post_tickets_flag',a)
a[Q]
POSTN_ptf <- POSTN[! POSTN %in% Q]
POSTN_ptf
a[POSTN_ptf]
dataset.ptf <- d[,-POSTN_ptf]
attach(dataset.ptf)
```

**Growing Trees**
Creating pure Classification Response Vector
```{r}
#Finding response vector
a.ptf <- colnames(dataset.ptf)
resp.ptf <- grepl("post_",a.ptf)
for(i in 1:length(a.ptf)){
  if(resp.ptf[i]){
    R <- i
  }
}
a.ptf[R]
#Converting it to a yes/no factor
y <- post_tickets_flag
Ticket <- ifelse(y<0.5,"No","Yes")
a.ptf[R]
dataset.ptf[,R] <- Ticket
dataset.ptf[,R] <- factor(dataset.ptf[,R])
attach(dataset.ptf)
rm(Ticket) #cleaning up environment space (keeping y as is for creaing boosted tree)
```

Growing Preliminary Classification Tree
```{r}
tree.ptf <- tree(post_tickets_flag~., dataset.ptf)
plot(tree.ptf)
text(tree.ptf,pretty=0)
```

Estimate Error Rate
```{r}
tree.ptf_train <- tree(post_tickets_flag~., dataset.ptf, subset=train)
plot(tree.ptf_train)
text(tree.ptf_train,pretty=0)
tree.ptf_pred <- predict(tree.ptf_train, dataset.ptf[-train,], type="class")
M <- table(tree.ptf_pred, post_tickets_flag[-train])
M
print(paste("The test error rate is", ((M[1,2]+M[2,1])/(sum(M)))))
```

Pruning the Tree
```{r}
cv.ptf_train <- cv.tree(tree.ptf_train, FUN=prune.misclass)
cv.ptf_train
b <- which.min(cv.ptf_train$dev)
print(paste("The best trees seem to be of size",cv.ptf_train$size[b],"with cross-validation error of",cv.ptf_train$dev[b],"."))
prune.ptf_train <- prune.misclass(tree.ptf_train, best=11)
plot(prune.ptf_train)
text(prune.ptf_train,pretty=0)
prune.ptf_pred <- predict(prune.ptf_train, dataset.ptf[-train,], type="class")
M_prune <- table(prune.ptf_pred, post_tickets_flag[-train])
M_prune
print(paste("The test error rate is", ((M_prune[1,2]+M_prune[2,1])/(sum(M_prune)))))
print("Same error rate.")
```

Creating Bagged Tree
```{r}
#Creating a modified dataset - na values are not accepted in bagged trees
dataset.ptf0 <- dataset.ptf
dataset.ptf0[is.na(dataset.ptf0)] <- 0
attach(dataset.ptf0)
bag.ptf_train <- randomForest(post_tickets_flag~., data=dataset.ptf0, subset=train, mtry=ncol(dataset.ptf0)-1, ntrees=1000, importance=TRUE)
bag.ptf_pred <- predict(bag.ptf_train, dataset.ptf0[-train,], type="class")
M_bag <- table(bag.ptf_pred, post_tickets_flag[-train])
M_bag
print(paste("The test error rate is", ((M_bag[1,2]+M_bag[2,1])/(sum(M_bag)))))
print("About 90% accurate!")
```

Looking at Predictor Importance 
```{r}
bag.I <- importance(bag.ptf_train)
bag.I <- bag.I[ order(bag.I[,3],decreasing=TRUE), ]
print("The fifteen most important predictors (by error) according to this model are as follows:")
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
bag.I <- bag.I[ order(bag.I[,4],decreasing=TRUE), ]
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating Random Forest Tree
```{r}
RF.ptf_train <- randomForest(post_tickets_flag~., data=dataset.ptf0, subset=train, ntrees=1000, importance=TRUE)
RF.ptf_pred <- predict(RF.ptf_train, dataset.ptf0[-train,], type="class")
M_RF <- table(RF.ptf_pred, post_tickets_flag[-train])
M_RF
print(paste("The test error rate is", ((M_RF[1,2]+M_bag[2,1])/(sum(M_RF)))))
print("More than 90% accurate!")
```

Looking at Predictor Importance 
```{r}
RF.I <- importance(RF.ptf_train)
RF.I <- RF.I[ order(RF.I[,3],decreasing=TRUE), ]
print("The fifteen most important predictors (by error) according to this model are as follows:")
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
RF.I <- RF.I[ order(RF.I[,4],decreasing=TRUE), ]
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating a Boosted Tree
```{r}
#Creating a modified dataset - columns with no variation are not accepted in boosted trees
dataset.ptfB <- dataset.ptf
colnames(dataset.ptfB)[c(36,56)]
dataset.ptfB <- dataset.ptfB[,-c(36,56)]
attach(dataset.ptfB)
#Modify response variable
dataset.ptfB$post_tickets_flag <- y
#distribution='bernoulli' requires 0,1 variable
boost.ptf_train <- gbm(post_tickets_flag~., data=dataset.ptfB[train,], distribution='bernoulli', n.trees=1000, interaction.depth=4)
boost.ptf_pred <- predict(boost.ptf_train, dataset.ptfB[-train,], n.trees=1000)
M_boost <- table(boost.ptf_pred, post_tickets_flag[-train])
M_boost
print(paste("The test error rate is", ((M_boost[1,2]+M_boost[2,1])/(sum(M_boost)))))
print("We might just never know how accurate this is...")
```

**B - How many tickets do people buy?**

Removing other response variables besides post_total_tickets
```{r}
Q <- match('post_total_tickets',a)
a[Q]
POSTN_ptt <- POSTN[! POSTN %in% Q]
POSTN_ptt
a[POSTN_ptt]
dataset.ptt <- d[,-POSTN_ptt]
attach(dataset.ptt)
```

**Growing Trees**

Growing Preliminary Regression Tree
```{r}
tree.ptt <- tree(post_total_tickets~., dataset.ptt)
plot(tree.ptt)
text(tree.ptt,pretty=0)
```

Estimate MSE
```{r}
tree.ptt_train <- tree(post_total_tickets~., dataset.ptt, subset=train)
plot(tree.ptt_train)
text(tree.ptt_train,pretty=0)
tree.ptt_pred <- predict(tree.ptt_train, dataset.ptt[-train,])
plot(tree.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.ptt <- mean((tree.ptt_pred-post_total_tickets[-train])^2)
MSE.ptt
print(paste("Test MSE is",MSE.ptt,". Root MSE is",sqrt(MSE.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.ptt),1),"total tickets purchased. This is pretty bad, considering the vast majority seem to have bought fewer than 20 or so tickets total."))
```

Pruning the Tree
```{r}
cv.ptt_train <- cv.tree(tree.ptt_train, FUN=prune.tree)
cv.ptt_train
b <- which.min(cv.ptt_train$dev)
print(paste("The best trees seem to be of size",cv.ptt_train$size[b],"with deviation of",cv.ptt_train$dev[b],"."))
prune.ptt_train <- prune.tree(tree.ptt_train, best=11)
plot(prune.ptt_train)
text(prune.ptt_train,pretty=0)
prune.ptt_pred <- predict(prune.ptt_train, dataset.ptt[-train,])
plot(prune.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.prune.ptt <- mean((prune.ptt_pred-post_total_tickets[-train])^2)
MSE.prune.ptt
print(paste("Test MSE is",MSE.prune.ptt,". Root MSE is",sqrt(MSE.prune.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.prune.ptt),1),"total tickets purchased. Successfully pruned, but still pretty bad."))
```

Creating Bagged Tree
```{r}
#Creating a modified dataset - na values are not accepted in bagged trees
dataset.ptt0 <- dataset.ptt
dataset.ptt0[is.na(dataset.ptt0)] <- 0
attach(dataset.ptt0)
bag.ptt_train <- randomForest(post_total_tickets~., data=dataset.ptt0, subset=train, mtry=ncol(dataset.ptt0)-1, ntrees=1000, importance=TRUE)
bag.ptt_pred <- predict(bag.ptt_train, dataset.ptt0[-train,])
plot(bag.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.bag.ptt <- mean((bag.ptt_pred-post_total_tickets[-train])^2)
MSE.bag.ptt
print(paste("Test MSE is",MSE.bag.ptt,". Root MSE is",sqrt(MSE.bag.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.bag.ptt),1),"total tickets purchased. No dice."))
```

Looking at Predictor Importance 
```{r}
bag.I <- importance(bag.ptf_train)
bag.I <- bag.I[ order(bag.I[,3],decreasing=TRUE), ]
print("The fifteen most important predictors (by error) according to this model are as follows:")
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
bag.I <- bag.I[ order(bag.I[,4],decreasing=TRUE), ]
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating Random Forest Tree
```{r}
RF.ptt_train <- randomForest(post_total_tickets~., data=dataset.ptt0, subset=train, importance=TRUE)
RF.ptt_pred <- predict(RF.ptt_train, dataset.ptt0[-train,])
plot(RF.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.RF.ptt <- mean((RF.ptt_pred-post_total_tickets[-train])^2)
MSE.RF.ptt
print(paste("Test MSE is",MSE.bag.ptt,". Root MSE is",sqrt(MSE.RF.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.RF.ptt),1),"total tickets purchased. Best of so far, nothing good yet."))
```

Looking at Predictor Importance 
```{r}
RF.I <- importance(RF.ptf_train)
RF.I <- RF.I[ order(RF.I[,3],decreasing=TRUE), ]
print("The fifteen most important predictors (by error) according to this model are as follows:")
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
RF.I <- RF.I[ order(RF.I[,4],decreasing=TRUE), ]
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating a Boosted Tree
```{r}
#Creating a modified dataset - columns with no variation are not accepted in boosted trees
dataset.pttB <- dataset.ptt
colnames(dataset.pttB)[c(36,56)]
dataset.pttB <- dataset.pttB[,-c(36,56)]
attach(dataset.pttB)
boost.ptt_train <- gbm(post_total_tickets~., data=dataset.pttB[train,], distribution='gaussian', n.trees=1000, interaction.depth=4)
boost.ptt_pred <- predict(boost.ptt_train, dataset.pttB[-train,], n.trees=1000)
plot(boost.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.boost.ptt <- mean((boost.ptt_pred-post_total_tickets[-train])^2)
MSE.boost.ptt
print(paste("Test MSE is",MSE.boost.ptt,". Root MSE is",sqrt(MSE.boost.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.boost.ptt),1),"total tickets purchased. Still not good enough."))

print("Seems like there's not enough useful data for this prediction.")
```

**C - How much money do people spend on tickets?**

Removing other response variables besides post_total_ticket_amt
```{r}
Q <- match('post_total_ticket_amt',a)
a[Q]
POSTN_ptta <- POSTN[! POSTN %in% Q]
POSTN_ptta
a[POSTN_ptta]
dataset.ptta <- d[,-POSTN_ptta]
attach(dataset.ptta)
```

**Growing Trees**

Growing Preliminary Regression Tree
```{r}
tree.ptta <- tree(post_total_ticket_amt~., dataset.ptta)
plot(tree.ptta)
text(tree.ptta,pretty=0)
```

Estimate MSE
```{r}
tree.ptta_train <- tree(post_total_ticket_amt~., dataset.ptta, subset=train)
plot(tree.ptta_train)
text(tree.ptta_train,pretty=0)
tree.ptta_pred <- predict(tree.ptta_train, dataset.ptta[-train,])
plot(tree.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.ptta <- mean((tree.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.ptta
print(paste("Test MSE is",MSE.ptta,". Root MSE is",sqrt(MSE.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.ptta),1),"total tickets purchased. This is pretty bad, considering the vast majority seem to have bought fewer than 20 or so tickets total."))
```

Pruning the Tree
```{r}
cv.ptta_train <- cv.tree(tree.ptta_train, FUN=prune.tree)
cv.ptta_train
b <- which.min(cv.ptta_train$dev)
print(paste("The best trees seem to be of size",cv.ptta_train$size[b],"with deviation of",cv.ptta_train$dev[b],"."))
prune.ptta_train <- prune.tree(tree.ptta_train, best=11)
plot(prune.ptta_train)
text(prune.ptta_train,pretty=0)
prune.ptta_pred <- predict(prune.ptta_train, dataset.ptta[-train,])
plot(prune.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.prune.ptta <- mean((prune.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.prune.ptta
print(paste("Test MSE is",MSE.prune.ptta,". Root MSE is",sqrt(MSE.prune.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.prune.ptta),1),"total tickets purchased. Successfully pruned, but still pretty bad."))
```

Creating Bagged Tree
```{r}
#Creating a modified dataset - na values are not accepted in bagged trees
dataset.ptta0 <- dataset.ptta
dataset.ptta0[is.na(dataset.ptta0)] <- 0
attach(dataset.ptta0)
bag.ptta_train <- randomForest(post_total_ticket_amt~., data=dataset.ptta0, subset=train, mtry=ncol(dataset.ptta0)-1, ntrees=1000, importance=TRUE)
bag.ptta_pred <- predict(bag.ptta_train, dataset.ptta0[-train,])
plot(bag.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.bag.ptta <- mean((bag.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.bag.ptta
print(paste("Test MSE is",MSE.bag.ptta,". Root MSE is",sqrt(MSE.bag.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.bag.ptta),1),"total tickets purchased. No dice."))
```

Looking at Predictor Importance 
```{r}
bag.I <- importance(bag.ptf_train)
bag.I <- bag.I[ order(bag.I[,3],decreasing=TRUE), ]
print("The fifteen most important predictors (by error) according to this model are as follows:")
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
bag.I <- bag.I[ order(bag.I[,4],decreasing=TRUE), ]
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating Random Forest Tree
```{r}
RF.ptta_train <- randomForest(post_total_ticket_amt~., data=dataset.ptta0, subset=train, importance=TRUE)
RF.ptta_pred <- predict(RF.ptta_train, dataset.ptta0[-train,])
plot(RF.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.RF.ptta <- mean((RF.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.RF.ptta
print(paste("Test MSE is",MSE.bag.ptta,". Root MSE is",sqrt(MSE.RF.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.RF.ptta),1),"total tickets purchased. Best of so far, nothing good yet."))
```

Looking at Predictor Importance 
```{r}
RF.I <- importance(RF.ptf_train)
RF.I <- RF.I[ order(RF.I[,3],decreasing=TRUE), ]
print("The fifteen most important predictors (by error) according to this model are as follows:")
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
RF.I <- RF.I[ order(RF.I[,4],decreasing=TRUE), ]
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating a Boosted Tree
```{r}
#Creating a modified dataset - columns with no variation are not accepted in boosted trees
dataset.pttaB <- dataset.ptta
colnames(dataset.pttaB)[c(36,56)]
dataset.pttaB <- dataset.pttaB[,-c(36,56)]
attach(dataset.pttaB)
boost.ptta_train <- gbm(post_total_ticket_amt~., data=dataset.pttaB[train,], distribution='gaussian', n.trees=1000, interaction.depth=4)
boost.ptta_pred <- predict(boost.ptta_train, dataset.pttaB[-train,], n.trees=1000)
plot(boost.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.boost.ptta <- mean((boost.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.boost.ptta
print(paste("Test MSE is",MSE.boost.ptta,". Root MSE is",sqrt(MSE.boost.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.boost.ptta),1),"total tickets purchased. Still not good enough."))

print("Seems like there's not enough useful data for this prediction.")
```

D - Who buys tickets at which price?
Better seats can be marketted more to those who are more likely to buy them.

