---
title: "NBA Project"
author: "Ilya Kukovitskiy"
date: "April 30, 2018"
output: html_document
---
```{r,results='hide'}
set.seed(1)
library(tree)
library(randomForest)
library(gbm)
library(ggplot2)
```

## Data Preprocessing

Reading in and storing dataset from file
```{r}
d <- read.csv("nbasample2GrpPlus0429.csv",header=T, sep=",")
```

Removing ax_ and la_ variales, as requested
```{r}
a <- colnames(d)
AX <- grepl("ax_",a) #seeing which values have prefix ax_
AXN <- c() #creating temporary vector recording positions
for(i in 1:length(a)){ #getting rejected values
  if(AX[i]){
    AXN <- c(AXN,i)
  }
}
d <- d[,-AXN] #removing rejected values
a <- colnames(d)
LA <- grepl("la_",a)
LAN <- c() #creating temporary vector recording positions
for(i in 1:length(a)){ #getting rejected values
  if(LA[i]){
    LAN <- c(LAN,i)
  }
}
d <- d[,-LAN] #removing rejected values
a <- colnames(d)
#Cleaning up Environment
rm(AX,AXN,LA,LAN)
```

Removing other Variables
```{r}
#closest_arena and closest_team are identical - removing closest_arena
x <- match("closest_arena",a)
#x
d <- d[,-x]
a <- colnames(d)
#match("closest_arena",a) #successfully removed

#names(Filter(is.factor, d)) #taking a look at which variables have factors - tree() can only handle 32 levels on factor predictors

#zip_team and zip_arena have 56 levels - too many to grow trees on - removing both
x <- match(c("zip_team","zip_arena"),a)
#x
d <- d[,-x]
a <- colnames(d)
#match(c("zip_team","zip_arena"),a) #successfully removed
#email_domain has 1468 levels - too many.. -removing
x <- match(c("email_domain"),a)
#x
d <- d[,-x]
a <- colnames(d)
#match(c("email_domain"),a) #successfully removed
#source_date has 3773 levels - too many.. -removing
x <- match(c("source_date"),a)
#x
d <- d[,-x]
a <- colnames(d)
#match(c("source_date"),a) #successfully removed
```

**Making a training/test set vector**

```{r}
train <- sample(1:nrow(d), nrow(d)*(.8))
```


**Viewing Response Variables**

```{r}
POST <- grepl("post_",a)
POSTN <- c()
for(i in 1:length(a)){ #getting rejected values
  if(POST[i]){
    POSTN <- c(POSTN,i)
  }
}
#POSTN
a[POSTN]
```

## Given this dataset, a few useful questions can be answered:

[**A** - Who buys tickets?](#begin_A)
Successful or unsuccessful marketting strategies can be identified.

[**B** - How many tickets do people buy?]($begin_B)
Besides success of strategy; if a person is likely to buy more tickets than they have purchased, they can likely be sold more tickets.

[**C** - How much money do people spend on tickets?]($begin_C)
Same rationale as B, but with dollar-value rather than number of games.

[**D** - Who buys tickets at which price?]($begin_D)
Better seats can be marketted more to those who are more likely to buy them.

### <a id="begin_A"></a>A - Who buys tickets?

Creating matrix to record model type and effectiveness
```{r}
A_methods <- matrix(c("",""),
                    nrow=1,
                    ncol=2)
colnames(A_methods) <- c("Method","Accuracy")
```

Removing other response variables besides post_tickets_flag
```{r}
Q <- match('post_tickets_flag',a)
a[Q]
POSTN_ptf <- POSTN[! POSTN %in% Q]
#POSTN_ptf
a[POSTN_ptf]
dataset.ptf <- d[,-POSTN_ptf]
```
```{r,results='hide'}
attach(dataset.ptf)
```

**Growing Trees**
Creating pure Classification Response Vector
```{r}
#Finding response vector
a.ptf <- colnames(dataset.ptf)
resp.ptf <- grepl("post_",a.ptf)
for(i in 1:length(a.ptf)){
  if(resp.ptf[i]){
    R <- i
  }
}
a.ptf[R]
#Converting it to a yes/no factor
y <- post_tickets_flag
Ticket <- ifelse(y<0.5,"No","Yes")
dataset.ptf[,R] <- Ticket
dataset.ptf[,R] <- factor(dataset.ptf[,R])
```
```{r,results='hide'}
attach(dataset.ptf)
```
```{r}
#Cleaning up environment space (keeping y as is for creaing boosted tree)
rm(Ticket) 
```

Growing Preliminary Classification Tree
```{r}
tree.ptf <- tree(post_tickets_flag~., dataset.ptf)
plot(tree.ptf)
text(tree.ptf,pretty=0)
```

Estimate and Record Error Rate on (extremely similar) Trained Tree
```{r}
tree.ptf_train <- tree(post_tickets_flag~., dataset.ptf, subset=train)
plot(tree.ptf_train)
text(tree.ptf_train,pretty=0)
tree.ptf_pred <- predict(tree.ptf_train, dataset.ptf[-train,], type="class")
M <- table(tree.ptf_pred, post_tickets_flag[-train])
M
A_methods <- rbind(A_methods, c('Tree',1-M[1,2]+M[2,1])/(sum(M)))
```
```{r,echo=FALSE}
print(paste("The test error rate is", ((M[1,2]+M[2,1])/(sum(M)))))
```

Pruning the Tree
```{r}
cv.ptf_train <- cv.tree(tree.ptf_train, FUN=prune.misclass)
cv.ptf_train
b <- which.min(cv.ptf_train$dev)
Best <- cv.ptf_train$size[b]
print(paste("The best trees seem to be of size",cv.ptf_train$size[b],"with cross-validation error of",cv.ptf_train$dev[b]))
prune.ptf_train <- prune.misclass(tree.ptf_train, best=Best)
plot(prune.ptf_train)
text(prune.ptf_train,pretty=0)
prune.ptf_pred <- predict(prune.ptf_train, dataset.ptf[-train,], type="class")
M_prune <- table(prune.ptf_pred, post_tickets_flag[-train])
M_prune
A_methods <- rbind(A_methods, c('Cross Validated Tree',1-M_prune[1,2]+M_prune[2,1])/(sum(M_prune)))
```
```{r,echo=FALSE}
print(paste("The test error rate is", ((M_prune[1,2]+M_prune[2,1])/(sum(M_prune)))))
print("Same error rate.")
```

Creating Bagged Tree
```{r}
#Creating a modified dataset - na values are not accepted in bagged trees
dataset.ptf0 <- dataset.ptf
dataset.ptf0[is.na(dataset.ptf0)] <- 0
```
```{r,results='hide'}
attach(dataset.ptf0)
```
```{r}
bag.ptf_train <- randomForest(post_tickets_flag~., data=dataset.ptf0, subset=train, mtry=ncol(dataset.ptf0)-1, ntrees=1000, importance=TRUE)
bag.ptf_pred <- predict(bag.ptf_train, dataset.ptf0[-train,], type="class")
M_bag <- table(bag.ptf_pred, post_tickets_flag[-train])
M_bag
A_methods <- rbind(A_methods, c('Bagged Tree',1-M_bag[1,2]+M_bag[2,1])/(sum(M_bag)))
```
```{r,echo=FALSE}
print(paste("The test error rate is", ((M_bag[1,2]+M_bag[2,1])/(sum(M_bag)))))
print("About 90% accurate!")
```

Looking at Predictor Importance 
```{r}
bag.I <- importance(bag.ptf_train)
bag.I <- bag.I[ order(bag.I[,3],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by error) according to this model are as follows:")
```
```{r}
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
bag.I <- bag.I[ order(bag.I[,4],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
```
```{r}
bag.I[1:15,] 
```

Creating Random Forest Tree
```{r}
RF.ptf_train <- randomForest(post_tickets_flag~., data=dataset.ptf0, subset=train, ntrees=1000, importance=TRUE)
RF.ptf_pred <- predict(RF.ptf_train, dataset.ptf0[-train,], type="class")
M_RF <- table(RF.ptf_pred, post_tickets_flag[-train])
M_RF
A_methods <- rbind(A_methods, c('Random Forest Tree',1-M_RF[1,2]+M_bag[2,1])/(sum(M_RF)))
```
```{r,echo=FALSE}
print(paste("The test error rate is", ((M_RF[1,2]+M_bag[2,1])/(sum(M_RF)))))
print("More than 90% accurate!")
```

Looking at Predictor Importance 
```{r}
RF.I <- importance(RF.ptf_train)
RF.I <- RF.I[ order(RF.I[,3],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by error) according to this model are as follows:")
```
```{r}
RF.I[1:15,] 
RF.I <- RF.I[ order(RF.I[,4],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
```
```{r}
RF.I[1:15,] 
```

Creating a Boosted Tree
```{r}
#Creating a modified dataset - columns with no variation are not accepted in boosted trees
dataset.ptfB <- dataset.ptf
colnames(dataset.ptfB)[c(36,56)]
dataset.ptfB <- dataset.ptfB[,-c(36,56)]
```
```{r,results='hide'}
attach(dataset.ptfB)
```
```{r}
#Modify response variable
dataset.ptfB$post_tickets_flag <- y
#distribution='bernoulli' requires 0,1 variable
boost.ptf_train <- gbm(post_tickets_flag~., data=dataset.ptfB[train,], distribution='bernoulli', n.trees=1000, interaction.depth=4)
boost.ptf_pred <- predict(boost.ptf_train, dataset.ptfB[-train,], n.trees=1000)
M_boost <- table(boost.ptf_pred, post_tickets_flag[-train])
A_methods <- rbind(A_methods, c('Boosted Tree',1-M_boost[1,2]+M_boost[2,1])/(sum(M_boost)))
```
```{r,echo=FALSE}
print(paste("The test error rate is", ((M_boost[1,2]+M_boost[2,1])/(sum(M_boost)))))
print("A bit worse, but still above 90% accuracy!")
```

Looking at Most Effective Method
```{r}
A_methods <- A_methods[-1,]
A_methods[order(A_methods$Accuracy),]
Top <- A_methods[1,]
```
```{r,echo=FALSE}
print(paste("The most effective method was the",Top[1],", with",Top[2],"accuracy."))
```
[Back to top](#top)

### <a id="begin_B"></a>B - How many tickets do people buy?

Removing other response variables besides post_total_tickets
```{r}
Q <- match('post_total_tickets',a)
a[Q]
POSTN_ptt <- POSTN[! POSTN %in% Q]
#POSTN_ptt
a[POSTN_ptt]
dataset.ptt <- d[,-POSTN_ptt]
```
```{r,results='hide'}
attach(dataset.ptt)
```

Looking at distribution of test responst variable
```{r}
#Creating Violin Plot
p <- ggplot(dataset.ptt[-train,], aes(y=post_total_tickets, 1))
p + geom_violin()
#Finding Number of Zeroes
O <- table(post_total_tickets[-train]==0)
#O[2]
print(paste(O[2]/(O[1]+O[2]),"of the values are zeroes."))
```

**Growing Trees**

Growing Preliminary Regression Tree
```{r}
tree.ptt <- tree(post_total_tickets~., dataset.ptt)
plot(tree.ptt)
text(tree.ptt,pretty=0)
```

Estimate MSE on (extremely similar) Trained Tree
```{r}
tree.ptt_train <- tree(post_total_tickets~., dataset.ptt, subset=train)
plot(tree.ptt_train)
text(tree.ptt_train,pretty=0)
tree.ptt_pred <- predict(tree.ptt_train, dataset.ptt[-train,])
plot(tree.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.ptt <- mean((tree.ptt_pred-post_total_tickets[-train])^2)
MSE.ptt
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.ptt,". Root MSE is",sqrt(MSE.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.ptt),2),"total tickets purchased."))
#fix Compare with distribution of response through box-plot
```

Pruning the Tree
```{r}
cv.ptt_train <- cv.tree(tree.ptt_train, FUN=prune.tree)
cv.ptt_train
b <- which.min(cv.ptt_train$dev)
Best <- cv.ptt_train$size[b]
print(paste("The best trees seem to be of size",cv.ptt_train$size[b],"with deviation of",cv.ptt_train$dev[b],"."))
prune.ptt_train <- prune.tree(tree.ptt_train, best=Best)
plot(prune.ptt_train)
text(prune.ptt_train,pretty=0)
prune.ptt_pred <- predict(prune.ptt_train, dataset.ptt[-train,])
plot(prune.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.prune.ptt <- mean((prune.ptt_pred-post_total_tickets[-train])^2)
MSE.prune.ptt
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.prune.ptt,". Root MSE is",sqrt(MSE.prune.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.prune.ptt),2),"total tickets purchased."))
```

Creating Bagged Tree
```{r}
#Creating a modified dataset - na values are not accepted in bagged trees
dataset.ptt0 <- dataset.ptt
dataset.ptt0[is.na(dataset.ptt0)] <- 0
```
```{r,results='hide'}
attach(dataset.ptt0)
```
```{r}
bag.ptt_train <- randomForest(post_total_tickets~., data=dataset.ptt0, subset=train, mtry=ncol(dataset.ptt0)-1, ntrees=1000, importance=TRUE)
bag.ptt_pred <- predict(bag.ptt_train, dataset.ptt0[-train,])
plot(bag.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.bag.ptt <- mean((bag.ptt_pred-post_total_tickets[-train])^2)
MSE.bag.ptt
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.bag.ptt,". Root MSE is",sqrt(MSE.bag.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.bag.ptt),2),"total tickets purchased."))
```

Looking at Predictor Importance 
```{r}
bag.I <- importance(bag.ptf_train)
bag.I <- bag.I[ order(bag.I[,3],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by error) according to this model are as follows:")
```
```{r}
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
bag.I <- bag.I[ order(bag.I[,4],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
```
```{r}
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating Random Forest Tree
```{r}
RF.ptt_train <- randomForest(post_total_tickets~., data=dataset.ptt0, subset=train, importance=TRUE)
RF.ptt_pred <- predict(RF.ptt_train, dataset.ptt0[-train,])
plot(RF.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.RF.ptt <- mean((RF.ptt_pred-post_total_tickets[-train])^2)
MSE.RF.ptt
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.bag.ptt,". Root MSE is",sqrt(MSE.RF.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.RF.ptt),2),"total tickets purchased."))
```

Looking at Predictor Importance 
```{r}
RF.I <- importance(RF.ptf_train)
RF.I <- RF.I[ order(RF.I[,3],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by error) according to this model are as follows:")
```
```{r}
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
RF.I <- RF.I[ order(RF.I[,4],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
```
```{r}
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating a Boosted Tree
```{r}
#Creating a modified dataset - columns with no variation are not accepted in boosted trees
dataset.pttB <- dataset.ptt
colnames(dataset.pttB)[c(36,56)]
dataset.pttB <- dataset.pttB[,-c(36,56)]
```
```{r,results='hide'}
attach(dataset.pttB)
```
```{r}
boost.ptt_train <- gbm(post_total_tickets~., data=dataset.pttB[train,], distribution='gaussian', n.trees=1000, interaction.depth=4)
boost.ptt_pred <- predict(boost.ptt_train, dataset.pttB[-train,], n.trees=1000)
plot(boost.ptt_pred, post_total_tickets[-train])
abline(0,1)
MSE.boost.ptt <- mean((boost.ptt_pred-post_total_tickets[-train])^2)
MSE.boost.ptt
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.boost.ptt,". Root MSE is",sqrt(MSE.boost.ptt),", indicating this model leads to test predictions within around",round(sqrt(MSE.boost.ptt),2),"total tickets purchased."))
```
[Back to top](#top)

### <a id="begin_C"></a>C - How much money do people spend on tickets?

Removing other response variables besides post_total_ticket_amt
```{r}
Q <- match('post_total_ticket_amt',a)
a[Q]
POSTN_ptta <- POSTN[! POSTN %in% Q]
#POSTN_ptta
a[POSTN_ptta]
dataset.ptta <- d[,-POSTN_ptta]
```
```{r,results='hide'}
attach(dataset.ptta)
```

Looking at distribution of test responst variable
```{r}
#Creating Violin Plot
p <- ggplot(dataset.ptta[-train,], aes(y=post_total_ticket_amt, 0))
p + geom_violin()
#Finding Number of Zeroes
O <- table(post_total_ticket_amt[-train]==0)
#O[2]
print(paste(O[2]/(O[1]+O[2]),"of the values are zeroes."))
```

**Growing Trees**

Growing Preliminary Regression Tree
```{r}
tree.ptta <- tree(post_total_ticket_amt~., dataset.ptta)
plot(tree.ptta)
text(tree.ptta,pretty=0)
```

Estimate MSE on (extremely similar) Trained Tree
```{r}
tree.ptta_train <- tree(post_total_ticket_amt~., dataset.ptta, subset=train)
plot(tree.ptta_train)
text(tree.ptta_train,pretty=0)
tree.ptta_pred <- predict(tree.ptta_train, dataset.ptta[-train,])
plot(tree.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.ptta <- mean((tree.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.ptta
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.ptta,". Root MSE is",sqrt(MSE.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.ptta),1),"dollars of total ticket amount."))
```

Pruning the Tree
```{r}
cv.ptta_train <- cv.tree(tree.ptta_train, FUN=prune.tree)
cv.ptta_train
b <- which.min(cv.ptta_train$dev)
Best <- cv.ptta_train$size[b]
print(paste("The best trees seem to be of size",cv.ptta_train$size[b],"with deviation of",cv.ptta_train$dev[b],"."))
prune.ptta_train <- prune.tree(tree.ptta_train, best=Best)
plot(prune.ptta_train)
text(prune.ptta_train,pretty=0)
prune.ptta_pred <- predict(prune.ptta_train, dataset.ptta[-train,])
plot(prune.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.prune.ptta <- mean((prune.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.prune.ptta
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.prune.ptta,". Root MSE is",sqrt(MSE.prune.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.prune.ptta),1),"dollars of total ticket amount."))
```

Creating Bagged Tree
```{r}
#Creating a modified dataset - na values are not accepted in bagged trees
dataset.ptta0 <- dataset.ptta
dataset.ptta0[is.na(dataset.ptta0)] <- 0
```
```{r,results='hide'}
attach(dataset.ptta0)
```
```{r}
bag.ptta_train <- randomForest(post_total_ticket_amt~., data=dataset.ptta0, subset=train, mtry=ncol(dataset.ptta0)-1, ntrees=1000, importance=TRUE)
bag.ptta_pred <- predict(bag.ptta_train, dataset.ptta0[-train,])
plot(bag.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.bag.ptta <- mean((bag.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.bag.ptta
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.bag.ptta,". Root MSE is",sqrt(MSE.bag.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.bag.ptta),1),"dollars of total ticket amount."))
```

Looking at Predictor Importance 
```{r}
bag.I <- importance(bag.ptf_train)
bag.I <- bag.I[ order(bag.I[,3],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by error) according to this model are as follows:")
```
```{r}
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
bag.I <- bag.I[ order(bag.I[,4],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
```
```{r}
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating Random Forest Tree
```{r}
RF.ptta_train <- randomForest(post_total_ticket_amt~., data=dataset.ptta0, subset=train, importance=TRUE)
RF.ptta_pred <- predict(RF.ptta_train, dataset.ptta0[-train,])
plot(RF.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.RF.ptta <- mean((RF.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.RF.ptta
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.bag.ptta,". Root MSE is",sqrt(MSE.RF.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.RF.ptta),1),"dollars of total ticket amount."))
```

Looking at Predictor Importance 
```{r}
RF.I <- importance(RF.ptf_train)
RF.I <- RF.I[ order(RF.I[,3],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by error) according to this model are as follows:")
```
```{r}
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
RF.I <- RF.I[ order(RF.I[,4],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
```
```{r}
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating a Boosted Tree
```{r}
#Creating a modified dataset - columns with no variation are not accepted in boosted trees
dataset.pttaB <- dataset.ptta
colnames(dataset.pttaB)[c(36,56)]
dataset.pttaB <- dataset.pttaB[,-c(36,56)]
```
```{r,results='hide'}
attach(dataset.pttaB)
```
```{r}
boost.ptta_train <- gbm(post_total_ticket_amt~., data=dataset.pttaB[train,], distribution='gaussian', n.trees=1000, interaction.depth=4)
boost.ptta_pred <- predict(boost.ptta_train, dataset.pttaB[-train,], n.trees=1000)
plot(boost.ptta_pred, post_total_ticket_amt[-train])
abline(0,1)
MSE.boost.ptta <- mean((boost.ptta_pred-post_total_ticket_amt[-train])^2)
MSE.boost.ptta
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.boost.ptta,". Root MSE is",sqrt(MSE.boost.ptta),", indicating this model leads to test predictions within around",round(sqrt(MSE.boost.ptta),1),"dollars of total ticket amount."))
```
[Back to top](#top)

### <a id="begin_D"></a>D - Who buys tickets at which price?

Removing other response variables besides post_avg_ticket_price
```{r}
Q <- match('post_avg_ticket_price',a)
a[Q]
POSTN_patp <- POSTN[! POSTN %in% Q]
#POSTN_patp
a[POSTN_patp]
dataset.patp <- d[,-POSTN_patp]
```
```{r,results='hide'}
attach(dataset.patp)
```

Looking at distribution of test response variable
```{r}
#Creating Violin Plot
p <- ggplot(dataset.patp[-train,], aes(y=post_avg_ticket_price, 0))
p + geom_violin()
#Finding Number of Zeroes
O <- table(post_avg_ticket_price[-train]==0)
#O[2]
print(paste(O[2]/(O[1]+O[2]),"of the values are zeroes."))
```

**Growing Trees**

Growing Preliminary Regression Tree
```{r}
tree.patp <- tree(post_avg_ticket_price~., dataset.patp)
plot(tree.patp)
text(tree.patp,pretty=0)
```

Estimate MSE on (extremely similar) Trained Tree
```{r}
tree.patp_train <- tree(post_avg_ticket_price~., dataset.patp, subset=train)
plot(tree.patp_train)
text(tree.patp_train,pretty=0)
tree.patp_pred <- predict(tree.patp_train, dataset.patp[-train,])
plot(tree.patp_pred, post_avg_ticket_price[-train])
abline(0,1)
MSE.patp <- mean((tree.patp_pred-post_avg_ticket_price[-train])^2)
MSE.patp
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.patp,". Root MSE is",sqrt(MSE.patp),", indicating this model leads to test predictions within around",round(sqrt(MSE.patp),1),"dollars of the average ticket price."))
```

Pruning the Tree
```{r}
cv.patp_train <- cv.tree(tree.patp_train, FUN=prune.tree)
cv.patp_train
b <- which.min(cv.patp_train$dev)
Best <- cv.patp_train$size[b]
```
```{r,echo=FALSE}
print(paste("The best trees seem to be of size",cv.patp_train$size[b],"with deviation of",cv.patp_train$dev[b],"."))
```
```{r}
prune.patp_train <- prune.tree(tree.patp_train, best=Best)
plot(prune.patp_train)
text(prune.patp_train,pretty=0)
prune.patp_pred <- predict(prune.patp_train, dataset.patp[-train,])
plot(prune.patp_pred, post_avg_ticket_price[-train])
abline(0,1)
MSE.prune.patp <- mean((prune.patp_pred-post_avg_ticket_price[-train])^2)
MSE.prune.patp
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.prune.patp,". Root MSE is",sqrt(MSE.prune.patp),", indicating this model leads to test predictions within around",round(sqrt(MSE.prune.patp),1),"dollars of the average ticket price."))
```

Creating Bagged Tree
```{r}
#Creating a modified dataset - na values are not accepted in bagged trees
dataset.patp0 <- dataset.patp
dataset.patp0[is.na(dataset.patp0)] <- 0
```
```{r,results='hide'}
attach(dataset.patp0)
```
```{r}
bag.patp_train <- randomForest(post_avg_ticket_price~., data=dataset.patp0, subset=train, mtry=ncol(dataset.patp0)-1, ntrees=1000, importance=TRUE)
bag.patp_pred <- predict(bag.patp_train, dataset.patp0[-train,])
plot(bag.patp_pred, post_avg_ticket_price[-train])
abline(0,1)
MSE.bag.patp <- mean((bag.patp_pred-post_avg_ticket_price[-train])^2)
MSE.bag.patp
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.bag.patp,". Root MSE is",sqrt(MSE.bag.patp),", indicating this model leads to test predictions within around",round(sqrt(MSE.bag.patp),1),"dollars of the average ticket price."))
```

Looking at Predictor Importance 
```{r}
bag.I <- importance(bag.ptf_train)
bag.I <- bag.I[ order(bag.I[,3],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by error) according to this model are as follows:")
```
```{r}
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
bag.I <- bag.I[ order(bag.I[,4],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
```
```{r}
bag.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating Random Forest Tree
```{r}
RF.patp_train <- randomForest(post_avg_ticket_price~., data=dataset.patp0, subset=train, importance=TRUE)
RF.patp_pred <- predict(RF.patp_train, dataset.patp0[-train,])
plot(RF.patp_pred, post_avg_ticket_price[-train])
abline(0,1)
MSE.RF.patp <- mean((RF.patp_pred-post_avg_ticket_price[-train])^2)
MSE.RF.patp
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.bag.patp,". Root MSE is",sqrt(MSE.RF.patp),", indicating this model leads to test predictions within around",round(sqrt(MSE.RF.patp),1),"dollars of the average ticket price."))
```

Looking at Predictor Importance 
```{r}
RF.I <- importance(RF.ptf_train)
RF.I <- RF.I[ order(RF.I[,3],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by error) according to this model are as follows:")
```
```{r}
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
RF.I <- RF.I[ order(RF.I[,4],decreasing=TRUE), ]
```
```{r,echo=FALSE}
print("The fifteen most important predictors (by Gini index) according to this model are as follows:")
```
```{r}
RF.I[1:15,] #includes variable 'tenure' - is this to be removed from this dataset as well?
```

Creating a Boosted Tree
```{r}
#Creating a modified dataset - columns with no variation are not accepted in boosted trees
dataset.patpB <- dataset.patp
colnames(dataset.patpB)[c(36,56)]
dataset.patpB <- dataset.patpB[,-c(36,56)]
```
```{r,results='hide'}
attach(dataset.patpB)
```
```{r}
boost.patp_train <- gbm(post_avg_ticket_price~., data=dataset.patpB[train,], distribution='gaussian', n.trees=1000, interaction.depth=4)
boost.patp_pred <- predict(boost.patp_train, dataset.patpB[-train,], n.trees=1000)
plot(boost.patp_pred, post_avg_ticket_price[-train])
abline(0,1)
MSE.boost.patp <- mean((boost.patp_pred-post_avg_ticket_price[-train])^2)
MSE.boost.patp
```
```{r,echo=FALSE}
print(paste("Test MSE is",MSE.boost.patp,". Root MSE is",sqrt(MSE.boost.patp),", indicating this model leads to test predictions within around",round(sqrt(MSE.boost.patp),1),"dollars of the average ticket price."))
```
[Back to top](#top)